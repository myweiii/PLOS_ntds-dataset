Four decades after the discovery of Ebola virus disease (EVD), the sources, reservoirs and dynamics of infection are still largely unknown and thus the threat of re-emergence remains ever present. As EVD thrives on fragile healthcare systems in the developing world, it is essential that triage tools are low-cost and easy-to-use in order to best allocate limited resources and ensure sustainability of EVD surveillance. From a public health perspective, sensitivity is paramount when screening for highly contagious and fatal diseases such as Ebola. However, once these suspect patients arrive at the treatment centres, specificity becomes far more important in order to accurately allocate them to risk-appropriate wards and better distribute limited resources. Currently, pre-test triage to identify “suspect” Ebola patients consists of a binary evaluation of non-specific symptoms that are shared by the much more prevalent disease: Malaria. Using these guidelines, over 70% of patients selected for admission to the potentially contagious environment of an ETC did not have Ebola. Within the ETC, patients may be further triaged into a higher risk “probable” ward on the basis of a clinically subjective assessment known as the “Ebola look”: since proven to have comparable accuracy to flipping a coin. While compartmentalising risk by stratification is an essential component to infection prevention and control measures, patient triage should be sufficiently accurate to justify to its benefit. This study constructs an easy-to-use and highly accurate (90%) triage scoring system that discriminates EVD infection risk in a malaria-sensitive manner: a strategy, which not only significantly improves the predictive accuracy for EVD but may also identify the (more deadly) infection of malaria.